{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94a66bce-1064-42a5-a654-f8344b644c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp313-cp313-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Downloading torch-2.6.0-cp313-none-macosx_11_0_arm64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: Pillow in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from sentence-transformers) (4.12.2)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.30.0->transformers)\n",
      "  Downloading fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting networkx (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (76.0.0)\n",
      "Collecting sympy==1.13.1 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch>=1.11.0->sentence-transformers)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "Downloading sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
      "Downloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Downloading regex-2024.11.6-cp313-cp313-macosx_11_0_arm64.whl (284 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl (418 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.6.0-cp313-none-macosx_11_0_arm64.whl (66.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Installing collected packages: mpmath, tqdm, sympy, safetensors, regex, networkx, fsspec, filelock, torch, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "Successfully installed filelock-3.18.0 fsspec-2025.3.2 huggingface-hub-0.30.2 mpmath-1.3.0 networkx-3.4.2 regex-2024.11.6 safetensors-0.5.3 sentence-transformers-4.1.0 sympy-1.13.1 tokenizers-0.21.1 torch-2.6.0 tqdm-4.67.1 transformers-4.51.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip3 install transformers sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9e24b6a-fd45-48bb-ae4a-91a6ab2b5d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "class EnhancedAgentWithPretrainedNLP:\n",
    "    def __init__(self):\n",
    "        self.goal = None\n",
    "        self.plan = []\n",
    "        self.current_step = 0\n",
    "        self.completed = False\n",
    "        self.history = []\n",
    "        self.entities = {}\n",
    "        self.intent = None\n",
    "        \n",
    "        # Load pre-trained models\n",
    "        print(\"Loading pre-trained NLP models...\")\n",
    "        # Zero-shot classification for intent\n",
    "        self.classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "        # Sentence embeddings for entity matching\n",
    "        self.sentence_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "        print(\"Models loaded successfully!\")\n",
    "        \n",
    "    def set_goal(self, goal_text):\n",
    "        \"\"\"Set the agent's goal and analyze it with pre-trained models\"\"\"\n",
    "        self.goal = goal_text\n",
    "        self.plan = []\n",
    "        self.current_step = 0\n",
    "        self.completed = False\n",
    "        self.history = [f\"Goal set: {goal_text}\"]\n",
    "        \n",
    "        # Analyze the goal text with pre-trained models\n",
    "        self.intent = self.classify_intent_with_model(goal_text)\n",
    "        self.entities = self.extract_entities_with_model(goal_text)\n",
    "        \n",
    "        return f\"Goal set: {goal_text}\\nDetected intent: {self.intent}\"\n",
    "    \n",
    "    def classify_intent_with_model(self, text):\n",
    "        \"\"\"Classify intent using a pre-trained zero-shot classification model\"\"\"\n",
    "        # Define possible intents\n",
    "        candidate_intents = [\"research\", \"scheduling\", \"writing\", \"analysis\", \"general\"]\n",
    "        \n",
    "        # Use the model to classify the text\n",
    "        result = self.classifier(text, candidate_intents)\n",
    "        \n",
    "        # Return the highest scoring intent\n",
    "        return result['labels'][0]\n",
    "    \n",
    "    def extract_entities_with_model(self, text):\n",
    "        \"\"\"Extract entities using semantic similarity with pre-defined entity types\"\"\"\n",
    "        entities = {\n",
    "            'topics': [],\n",
    "            'timeframes': [],\n",
    "            'quantities': []\n",
    "        }\n",
    "        \n",
    "        # For simplicity, we'll still use regex for timeframes and quantities\n",
    "        import re\n",
    "        \n",
    "        # Extract timeframes\n",
    "        timeframe_patterns = [\n",
    "            r'(?:by|before|after|during|for)\\s+([A-Za-z]+\\s+\\d+(?:st|nd|rd|th)?)',\n",
    "            r'(?:next|this|coming)\\s+([A-Za-z]+)',\n",
    "            r'(?:in)\\s+(\\d+)\\s+(?:days|weeks|months)'\n",
    "        ]\n",
    "        \n",
    "        for pattern in timeframe_patterns:\n",
    "            matches = re.findall(pattern, text.lower())\n",
    "            for match in matches:\n",
    "                if match.strip():\n",
    "                    entities['timeframes'].append(match.strip())\n",
    "        \n",
    "        # Extract quantities\n",
    "        quantity_patterns = [\n",
    "            r'(\\d+)\\s+(?:pages|items|points|sections|paragraphs)'\n",
    "        ]\n",
    "        \n",
    "        for pattern in quantity_patterns:\n",
    "            matches = re.findall(pattern, text.lower())\n",
    "            for match in matches:\n",
    "                if match.strip():\n",
    "                    entities['quantities'].append(match.strip())\n",
    "        \n",
    "        # For topics, we'll use a more sophisticated approach with sentence embeddings\n",
    "        # First, split the text into potential topic phrases\n",
    "        sentences = re.split(r'[.,;!?]|\\band\\b|\\bor\\b|\\bthe\\b|\\ba\\b|\\ban\\b|\\bto\\b|\\bfor\\b|\\bin\\b|\\bon\\b|\\bwith\\b', text)\n",
    "        sentences = [s.strip() for s in sentences if len(s.strip()) > 3]\n",
    "        \n",
    "        # Define some topic indicators\n",
    "        topic_indicators = [\n",
    "            \"about\", \"regarding\", \"concerning\", \"on the subject of\", \n",
    "            \"related to\", \"focusing on\", \"in relation to\"\n",
    "        ]\n",
    "        \n",
    "        # Encode sentences and topic indicators\n",
    "        if sentences:\n",
    "            sentence_embeddings = self.sentence_model.encode(sentences)\n",
    "            indicator_embeddings = self.sentence_model.encode(topic_indicators)\n",
    "            \n",
    "            # Find sentences that are semantically similar to topic indicators\n",
    "            for i, sentence_emb in enumerate(sentence_embeddings):\n",
    "                # Calculate similarity with topic indicators\n",
    "                similarities = util.pytorch_cos_sim(\n",
    "                    torch.tensor([sentence_emb]), \n",
    "                    torch.tensor(indicator_embeddings)\n",
    "                )[0]\n",
    "                \n",
    "                max_sim = torch.max(similarities).item()\n",
    "                \n",
    "                # If the sentence is similar to a topic indicator, the next phrase might be a topic\n",
    "                if max_sim > 0.3 and i < len(sentences) - 1:\n",
    "                    potential_topic = sentences[i+1].strip()\n",
    "                    if potential_topic and len(potential_topic) > 3 and potential_topic not in entities['topics']:\n",
    "                        entities['topics'].append(potential_topic)\n",
    "                \n",
    "                # Also check if the current sentence itself might be a topic\n",
    "                # This helps when the goal directly mentions the topic\n",
    "                words = sentences[i].split()\n",
    "                if len(words) >= 2 and len(words) <= 5:  # Reasonable length for a topic\n",
    "                    potential_topic = sentences[i].strip()\n",
    "                    if potential_topic and potential_topic not in entities['topics']:\n",
    "                        entities['topics'].append(potential_topic)\n",
    "        \n",
    "        # If no topics were found, try extracting nouns as potential topics\n",
    "        if not entities['topics']:\n",
    "            import nltk\n",
    "            try:\n",
    "                nltk.data.find('tokenizers/punkt')\n",
    "            except LookupError:\n",
    "                nltk.download('punkt')\n",
    "            try:\n",
    "                nltk.data.find('taggers/averaged_perceptron_tagger')\n",
    "            except LookupError:\n",
    "                nltk.download('averaged_perceptron_tagger')\n",
    "            \n",
    "            tokens = nltk.word_tokenize(text)\n",
    "            tagged = nltk.pos_tag(tokens)\n",
    "            \n",
    "            # Extract nouns (NN, NNS, NNP, NNPS)\n",
    "            nouns = [word for word, pos in tagged if pos.startswith('NN')]\n",
    "            \n",
    "            if nouns:\n",
    "                entities['topics'].append(' '.join(nouns[:2]))  # Just use the first couple of nouns\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    # The rest of the methods remain the same as in the EnhancedAgent class\n",
    "    def create_plan(self):\n",
    "        \"\"\"Create a context-aware plan based on intent and entities\"\"\"\n",
    "        if not self.goal:\n",
    "            return \"Please set a goal first.\"\n",
    "        \n",
    "        # Create plan based on intent and extracted entities\n",
    "        if self.intent == 'research':\n",
    "            topics = self.entities['topics'] or [\"the requested subject\"]\n",
    "            topic_str = ', '.join(topics)\n",
    "            \n",
    "            self.plan = [\n",
    "                f\"Search for information about {topic_str}\",\n",
    "                f\"Identify key aspects of {topic_str}\",\n",
    "                \"Organize findings into a coherent structure\",\n",
    "                \"Prepare a comprehensive summary\"\n",
    "            ]\n",
    "        \n",
    "        elif self.intent == 'scheduling':\n",
    "            timeframe = self.entities['timeframes'][0] if self.entities['timeframes'] else \"the specified period\"\n",
    "            \n",
    "            self.plan = [\n",
    "                f\"Identify all tasks for {timeframe}\",\n",
    "                \"Determine priorities and dependencies\",\n",
    "                \"Allocate time slots for each task\",\n",
    "                \"Create a structured schedule\"\n",
    "            ]\n",
    "        \n",
    "        elif self.intent == 'writing':\n",
    "            topic = self.entities['topics'][0] if self.entities['topics'] else \"the requested topic\"\n",
    "            \n",
    "            self.plan = [\n",
    "                f\"Outline the main points for {topic}\",\n",
    "                \"Develop an introduction and structure\",\n",
    "                \"Write the main content\",\n",
    "                \"Review and refine the final text\"\n",
    "            ]\n",
    "        \n",
    "        elif self.intent == 'analysis':\n",
    "            topic = self.entities['topics'][0] if self.entities['topics'] else \"the subject\"\n",
    "            \n",
    "            self.plan = [\n",
    "                f\"Gather data about {topic}\",\n",
    "                \"Identify patterns and relationships\",\n",
    "                \"Evaluate implications and significance\",\n",
    "                \"Formulate conclusions and recommendations\"\n",
    "            ]\n",
    "        \n",
    "        else:\n",
    "            # Default general plan\n",
    "            self.plan = [\n",
    "                \"Analyze the request requirements\",\n",
    "                \"Gather necessary information\",\n",
    "                \"Process the information systematically\",\n",
    "                \"Prepare appropriate response\"\n",
    "            ]\n",
    "        \n",
    "        plan_text = \"Plan created:\\n\" + \"\\n\".join([f\"{i+1}. {step}\" for i, step in enumerate(self.plan)])\n",
    "        self.history.append(plan_text)\n",
    "        return plan_text\n",
    "    \n",
    "    def execute_step(self):\n",
    "        \"\"\"Execute the current step with natural language generation\"\"\"\n",
    "        if not self.plan:\n",
    "            return \"No plan exists. Please create a plan first.\"\n",
    "            \n",
    "        if self.completed:\n",
    "            return \"All steps have been completed.\"\n",
    "            \n",
    "        current = self.plan[self.current_step]\n",
    "        \n",
    "        # Generate a natural language response based on the step type\n",
    "        response = self.generate_step_response(self.current_step, current)\n",
    "        \n",
    "        self.current_step += 1\n",
    "        if self.current_step >= len(self.plan):\n",
    "            self.completed = True\n",
    "            final_result = f\"{response}\\nAll steps completed!\"\n",
    "            self.history.append(final_result)\n",
    "            return final_result\n",
    "        \n",
    "        next_step = f\"{response}\\nNext step: {self.plan[self.current_step]}\"\n",
    "        self.history.append(next_step)\n",
    "        return next_step\n",
    "    \n",
    "    def generate_step_response(self, step_index, step_text):\n",
    "        \"\"\"Generate a natural language response for a step execution\"\"\"\n",
    "        # Templates for different step types\n",
    "        search_templates = [\n",
    "            \"I've gathered information about {topic}. The main sources include recent articles and trusted references.\",\n",
    "            \"Research complete on {topic}. I found several relevant resources with up-to-date information.\",\n",
    "            \"I've collected data on {topic} from multiple sources to ensure comprehensive coverage.\"\n",
    "        ]\n",
    "        \n",
    "        analysis_templates = [\n",
    "            \"I've analyzed the information and identified {count} key points about {topic}.\",\n",
    "            \"The data has been processed and organized into main categories for better understanding.\",\n",
    "            \"Analysis complete. I've structured the information to highlight the most important aspects.\"\n",
    "        ]\n",
    "        \n",
    "        creation_templates = [\n",
    "            \"I've prepared a complete summary that covers all the essential aspects of {topic}.\",\n",
    "            \"The final output is ready, organized in a clear and logical structure.\",\n",
    "            \"I've created a comprehensive response that addresses all parts of your request.\"\n",
    "        ]\n",
    "        \n",
    "        # Determine template category based on step keywords\n",
    "        if any(word in step_text.lower() for word in ['search', 'find', 'identify', 'gather', 'collect']):\n",
    "            templates = search_templates\n",
    "        elif any(word in step_text.lower() for word in ['analyze', 'organize', 'process', 'determine', 'develop']):\n",
    "            templates = analysis_templates\n",
    "        else:\n",
    "            templates = creation_templates\n",
    "        \n",
    "        # Extract topic if available\n",
    "        topic = self.entities['topics'][0] if self.entities['topics'] else \"the requested subject\"\n",
    "        \n",
    "        # Select a template and fill it\n",
    "        import random\n",
    "        template = random.choice(templates)\n",
    "        response = template.format(topic=topic, count=random.randint(3, 7))\n",
    "        \n",
    "        # Add progress information\n",
    "        progress = f\"Step {step_index + 1}/{len(self.plan)} completed\"\n",
    "        \n",
    "        return f\"{response}\\n{progress}\"\n",
    "    \n",
    "    def get_status(self):\n",
    "        \"\"\"Get the current status of the agent\"\"\"\n",
    "        if not self.goal:\n",
    "            return \"No goal set.\"\n",
    "        if not self.plan:\n",
    "            return f\"Goal: {self.goal}\\nIntent: {self.intent}\\nNo plan created yet.\"\n",
    "        if self.completed:\n",
    "            return f\"Goal: {self.goal}\\nIntent: {self.intent}\\nStatus: Completed all {len(self.plan)} steps.\"\n",
    "        \n",
    "        progress = f\"{self.current_step}/{len(self.plan)} steps completed\"\n",
    "        next_step = f\"Next: {self.plan[self.current_step]}\" if self.current_step < len(self.plan) else \"All done!\"\n",
    "        return f\"Goal: {self.goal}\\nIntent: {self.intent}\\nProgress: {progress}\\n{next_step}\"\n",
    "    \n",
    "    def get_history(self):\n",
    "        \"\"\"Get the full history of agent actions\"\"\"\n",
    "        return \"\\n\\n\".join(self.history)\n",
    "    \n",
    "    def get_nlp_analysis(self):\n",
    "        \"\"\"Return the NLP analysis of the current goal\"\"\"\n",
    "        if not self.goal:\n",
    "            return \"No goal set.\"\n",
    "        \n",
    "        topic_str = \", \".join(self.entities['topics']) if self.entities['topics'] else \"None detected\"\n",
    "        timeframe_str = \", \".join(self.entities['timeframes']) if self.entities['timeframes'] else \"None detected\"\n",
    "        quantity_str = \", \".join(self.entities['quantities']) if self.entities['quantities'] else \"None detected\"\n",
    "        \n",
    "        return f\"\"\"NLP Analysis (Using Pre-trained Models):\n",
    "Goal: \"{self.goal}\"\n",
    "Detected Intent: {self.intent}\n",
    "Extracted Entities:\n",
    "  - Topics: {topic_str}\n",
    "  - Timeframes: {timeframe_str}\n",
    "  - Quantities: {quantity_str}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66afa0e8-94d3-42d9-a203-012c86f45017",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>Agentic AI with Pre-trained NLP Models</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p>This notebook demonstrates an agentic AI using Hugging Face Transformers and Sentence Transformers for advanced NLP capabilities.</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h2>Example: Complex Goal with Pre-trained NLP Analysis</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 GOAL: \"Research the environmental impact of renewable energy sources and prepare a summary for my presentation next week\"\n",
      "--------------------------------------------------------------------------------\n",
      "STEP 1: Setting Goal & Advanced NLP Analysis\n",
      "  Goal set: Research the environmental impact of renewable energy sources and prepare a summary for my presentation next week\n",
      "Detected intent: research\n",
      "\n",
      "NLP ANALYSIS (Using Pre-trained Models):\n",
      "  NLP Analysis (Using Pre-trained Models):\n",
      "Goal: \"Research the environmental impact of renewable energy sources and prepare a summary for my presentation next week\"\n",
      "Detected Intent: research\n",
      "Extracted Entities:\n",
      "  - Topics: environmental impact of renewable energy sources, summary, my presentation next week\n",
      "  - Timeframes: week\n",
      "  - Quantities: None detected\n",
      "\n",
      "STEP 2: Creating Context-Aware Plan\n",
      "  Plan created:\n",
      "1. Search for information about environmental impact of renewable energy sources, summary, my presentation next week\n",
      "2. Identify key aspects of environmental impact of renewable energy sources, summary, my presentation next week\n",
      "3. Organize findings into a coherent structure\n",
      "4. Prepare a comprehensive summary\n",
      "\n",
      "STEP 3: Executing Plan with Natural Language Generation\n",
      "  Step 1: Research complete on environmental impact of renewable energy sources. I found several relevant resources with up-to-date information.\n",
      "Step 1/4 completed\n",
      "Next step: Identify key aspects of environmental impact of renewable energy sources, summary, my presentation next week\n",
      "\n",
      "  Step 2: Research complete on environmental impact of renewable energy sources. I found several relevant resources with up-to-date information.\n",
      "Step 2/4 completed\n",
      "Next step: Organize findings into a coherent structure\n",
      "\n",
      "  Step 3: I've gathered information about environmental impact of renewable energy sources. The main sources include recent articles and trusted references.\n",
      "Step 3/4 completed\n",
      "Next step: Prepare a comprehensive summary\n",
      "\n",
      "  Step 4: The final output is ready, organized in a clear and logical structure.\n",
      "Step 4/4 completed\n",
      "All steps completed!\n",
      "\n",
      "STEP 4: Final Status\n",
      "  Goal: Research the environmental impact of renewable energy sources and prepare a summary for my presentation next week\n",
      "Intent: research\n",
      "Status: Completed all 4 steps.\n",
      "\n",
      "📊 EXECUTION SUMMARY:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Step</th>\n",
       "      <th>Plan</th>\n",
       "      <th>Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Search for information about environmental imp...</td>\n",
       "      <td>Research complete on environmental impact of r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Identify key aspects of environmental impact o...</td>\n",
       "      <td>Research complete on environmental impact of r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Organize findings into a coherent structure</td>\n",
       "      <td>I've gathered information about environmental ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Prepare a comprehensive summary</td>\n",
       "      <td>The final output is ready, organized in a clea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Step                                               Plan  \\\n",
       "0     1  Search for information about environmental imp...   \n",
       "1     2  Identify key aspects of environmental impact o...   \n",
       "2     3        Organize findings into a coherent structure   \n",
       "3     4                    Prepare a comprehensive summary   \n",
       "\n",
       "                                              Result  \n",
       "0  Research complete on environmental impact of r...  \n",
       "1  Research complete on environmental impact of r...  \n",
       "2  I've gathered information about environmental ...  \n",
       "3  The final output is ready, organized in a clea...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate the agent with pre-trained NLP models\n",
    "def demonstrate_pretrained_agent(goal):\n",
    "    \"\"\"Run a complete agent cycle with pre-trained NLP models\"\"\"\n",
    "    print(f\"🎯 GOAL: \\\"{goal}\\\"\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Create a new agent (only do this once as model loading takes time)\n",
    "    if 'pretrained_agent' not in globals():\n",
    "        global pretrained_agent\n",
    "        pretrained_agent = EnhancedAgentWithPretrainedNLP()\n",
    "    \n",
    "    agent = pretrained_agent\n",
    "    \n",
    "    # Step 1: Set Goal and analyze with NLP\n",
    "    print(\"STEP 1: Setting Goal & Advanced NLP Analysis\")\n",
    "    result = agent.set_goal(goal)\n",
    "    print(f\"  {result}\")\n",
    "    print(\"\\nNLP ANALYSIS (Using Pre-trained Models):\")\n",
    "    print(f\"  {agent.get_nlp_analysis()}\")\n",
    "    print()\n",
    "    \n",
    "    # Step 2: Create Context-Aware Plan\n",
    "    print(\"STEP 2: Creating Context-Aware Plan\")\n",
    "    plan_result = agent.create_plan()\n",
    "    print(f\"  {plan_result}\")\n",
    "    print()\n",
    "    \n",
    "    # Step 3: Execute Steps with NLG\n",
    "    print(\"STEP 3: Executing Plan with Natural Language Generation\")\n",
    "    step_results = []\n",
    "    \n",
    "    # Execute all steps\n",
    "    for i in range(len(agent.plan)):\n",
    "        step_result = agent.execute_step()\n",
    "        step_results.append(step_result)\n",
    "        print(f\"  Step {i+1}: {step_result}\")\n",
    "        print()\n",
    "    \n",
    "    # Step 4: Final Status\n",
    "    print(\"STEP 4: Final Status\")\n",
    "    status = agent.get_status()\n",
    "    print(f\"  {status}\")\n",
    "    print()\n",
    "    \n",
    "    # Create a summary table\n",
    "    import pandas as pd\n",
    "    print(\"📊 EXECUTION SUMMARY:\")\n",
    "    summary_data = []\n",
    "    for i, (plan_step, result) in enumerate(zip(agent.plan, step_results)):\n",
    "        summary_data.append({\n",
    "            \"Step\": i+1,\n",
    "            \"Plan\": plan_step,\n",
    "            \"Result\": result.split('\\n')[0]  # Just take the first line of the result\n",
    "        })\n",
    "    \n",
    "    # Display as a formatted table\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    display(summary_df)\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    print()\n",
    "\n",
    "# Display a title\n",
    "display(HTML(\"<h1>Agentic AI with Pre-trained NLP Models</h1>\"))\n",
    "display(HTML(\"<p>This notebook demonstrates an agentic AI using Hugging Face Transformers and Sentence Transformers for advanced NLP capabilities.</p>\"))\n",
    "\n",
    "# Run a demonstration with a complex goal\n",
    "display(HTML(\"<h2>Example: Complex Goal with Pre-trained NLP Analysis</h2>\"))\n",
    "demonstrate_pretrained_agent(\"Research the environmental impact of renewable energy sources and prepare a summary for my presentation next week\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c3de04-0990-4bf3-91fd-2b3c05029f36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
